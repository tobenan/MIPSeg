{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 1, 1],\n",
       "         [1, 0, 1],\n",
       "         [0, 1, 1],\n",
       "         [0, 1, 0],\n",
       "         [1, 1, 1]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 0, 1, 0, 0, 0, 1],\n",
       "         [1, 1, 1, 0, 1, 1, 0, 1],\n",
       "         [1, 1, 1, 0, 1, 0, 1, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 0, 1, 0, 0, 1, 1, 1],\n",
       "         [1, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 1]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor([[1,1,1],[1,0,1],[0,1,1],[0,1,0],[1,1,1]])\n",
    "g= torch.randint(2,(8,8))\n",
    "g[0]=torch.ones(8)\n",
    "g[:,0]= torch.ones(8)\n",
    "a,g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 0, 1, 0, 0, 0, 1],\n",
       "        [1, 1, 1, 0, 1, 1, 0, 1],\n",
       "        [1, 1, 1, 0, 1, 0, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 0, 1, 0, 0, 1, 1, 1],\n",
       "        [1, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool= g!=0\n",
    "(bool).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ True, False, False, False, False, False, False, False]),\n",
       " tensor([0, 1, 1, 1, 1, 4, 2, 1]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row,indice=torch.min(g,axis=1)\n",
    "row,indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\"argmin_cpu\" not implemented for 'Bool'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m minrow\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39;49margmin(row)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"argmin_cpu\" not implemented for 'Bool'"
     ]
    }
   ],
   "source": [
    "minrow=torch.argmin(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 1, 2, 3, 3, 1, 1]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row,indice=torch.min(g,axis=1)\n",
    "row,indice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1), tensor(7))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minrow=torch.argmin(row)\n",
    "maxrow = row.shape[0]-torch.argmin(row.flip(0))-1\n",
    "minrow,maxrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 0, 0, 0, 0, 0, 0, 0]), tensor(1), tensor(7))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col, _ =torch.min(g,axis=0)\n",
    "mincol=torch.argmin(col)\n",
    "maxcol=col.shape[0]-torch.argmin(col.flip(0))-1\n",
    "col,mincol,maxcol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 1, 1, 0, 1],\n",
       "        [0, 1, 1, 0, 0, 1, 0],\n",
       "        [1, 0, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 0, 1, 1, 1, 0],\n",
       "        [1, 1, 0, 0, 1, 1, 0],\n",
       "        [0, 1, 0, 1, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa=g[minrow:maxrow+1,mincol:maxcol+1]\n",
    "aa# 查看截取位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 2],\n",
       " tensor([[255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
       "         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
       "         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
       "         [255, 255,   1,   1,   1,   1,   1,   1,   1,   1, 255, 255],\n",
       "         [255, 255,   1,   0,   1,   0,   1,   1,   0,   1, 255, 255],\n",
       "         [255, 255,   1,   0,   1,   1,   0,   0,   1,   0, 255, 255],\n",
       "         [255, 255,   1,   1,   0,   1,   1,   1,   0,   0, 255, 255],\n",
       "         [255, 255,   1,   1,   1,   0,   1,   1,   1,   0, 255, 255],\n",
       "         [255, 255,   1,   1,   1,   0,   0,   1,   1,   0, 255, 255],\n",
       "         [255, 255,   1,   0,   1,   0,   1,   0,   1,   0, 255, 255],\n",
       "         [255, 255,   1,   0,   0,   1,   0,   1,   1,   0, 255, 255],\n",
       "         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
       "         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
       "         [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255]]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h=6 \n",
    "w=5\n",
    "pad_w_list = [int(w/2), int(w/2)] # 避免 w为奇数 除不尽的情况 如果为1 或0怎么处理 为0 后续则不取该图像 w-int(w/2)\n",
    "# h=6时 中心点上取三行 中心点下取两行 h=7时 中心点上取三行 下取三行 可以少pad一行一列 [int(w/2), int(w/2)]\n",
    "pad_h_list = [int(h/2), int(h/2)]\n",
    "padding = torch.nn.ConstantPad2d((pad_w_list[0],pad_w_list[1],pad_h_list[0],pad_h_list[1]), 255)\n",
    "gg=padding(g)\n",
    "pad_w_list,gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(10), tensor(7), tensor(13), tensor(7), tensor(10), tensor(5))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center_y = torch.randint( int(h/2) + minrow, int(h/2) + maxrow + 1,[] ) # get center point 均匀分布 还是正态分布\n",
    "center_x = torch.randint( int(w/2) + mincol, int(w/2) + maxcol + 1, [] ) \n",
    "                                                                                                \n",
    "# 截取区域\n",
    "new_max_row = center_y + (h-int(h/2))\n",
    "new_min_row = center_y - int(h/2)\n",
    "new_max_col = center_x + (w-int(w/2)) # 不需减一 因为后续提取位置会加一 \n",
    "new_min_col = center_x - int(w/2)\n",
    "center_y,center_x,new_max_row,new_min_row,new_max_col,new_min_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 1, 1, 0, 1],\n",
       "        [0, 1, 1, 0, 0, 1, 0],\n",
       "        [1, 0, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 0, 1, 1, 1, 0],\n",
       "        [1, 1, 0, 0, 1, 1, 0],\n",
       "        [0, 1, 0, 1, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa=gg[ int(h/2)+minrow:int(h/2)+maxrow+1,int(w/2)+mincol:int(w/2)+maxcol+1] # 目标范围\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   1,   1,   0],\n",
       "        [  0,   0,   1,   1,   0],\n",
       "        [  0,   1,   0,   1,   0],\n",
       "        [  1,   0,   1,   1,   0],\n",
       "        [255, 255, 255, 255, 255],\n",
       "        [255, 255, 255, 255, 255]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ggg=gg[new_min_row:new_max_row,new_min_col:new_max_col]\n",
    "ggg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1., 10.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### attentive\n",
    "features = torch.ones(4,4)\n",
    "features[1,2] =10 \n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_idx = torch.argmax(features)\n",
    "att_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, tensor(1), tensor(2))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_size = features.size(1) # 获取列的宽度\n",
    "row = torch.div(att_idx,att_size,rounding_mode='floor')\n",
    "col = att_idx % att_size\n",
    "att_size,row,col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_idx = torch.cat([ row.unsqueeze(0),   #[100,1,27] 除数 实际是反flatten 代表行数\n",
    "          col.unsqueeze(0),], dim=0)      #[100,1,27] 余数 代表列数 [4，2] 如果设计为tok 则为[4,2,tok]\n",
    "att_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 2],\n",
       " tensor([[255., 255., 255., 255., 255., 255., 255., 255.],\n",
       "         [255., 255., 255., 255., 255., 255., 255., 255.],\n",
       "         [255., 255., 255., 255., 255., 255., 255., 255.],\n",
       "         [255., 255.,   1.,   1.,   1.,   1., 255., 255.],\n",
       "         [255., 255.,   1.,   1.,  10.,   1., 255., 255.],\n",
       "         [255., 255.,   1.,   1.,   1.,   1., 255., 255.],\n",
       "         [255., 255.,   1.,   1.,   1.,   1., 255., 255.],\n",
       "         [255., 255., 255., 255., 255., 255., 255., 255.],\n",
       "         [255., 255., 255., 255., 255., 255., 255., 255.],\n",
       "         [255., 255., 255., 255., 255., 255., 255., 255.]]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h=6 \n",
    "w=5\n",
    "pad_w_list = [int(w/2), int(w/2)] # 避免 w为奇数 除不尽的情况 如果为1 或0怎么处理 为0 后续则不取该图像 w-int(w/2)\n",
    "# h=6时 中心点上取三行 中心点下取两行 h=7时 中心点上取三行 下取三行 可以少pad一行一列 [int(w/2), int(w/2)]\n",
    "pad_h_list = [int(h/2), int(h/2)]\n",
    "padding = torch.nn.ConstantPad2d((pad_w_list[0],pad_w_list[1],pad_h_list[0],pad_h_list[1]), 255)\n",
    "pad_F=padding(features)\n",
    "pad_w_list, pad_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4), tensor(4))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center_y = att_idx[ 0]+int(h/2)\n",
    "center_x = att_idx[ 1]+int(w/2)\n",
    "center_x,center_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(7), tensor(2), tensor(7), tensor(1))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_max_row = center_y + (h-int(h/2))\n",
    "new_min_row = center_y - int(h/2)\n",
    "new_max_col = center_x + (w-int(w/2)) # 不需减一 因为后续提取位置会加一 \n",
    "new_min_col = center_x - int(w/2)\n",
    "new_max_col,new_min_col,new_max_row,new_min_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[255., 255., 255., 255., 255.],\n",
       "        [255., 255., 255., 255., 255.],\n",
       "        [  1.,   1.,   1.,   1., 255.],\n",
       "        [  1.,   1.,  10.,   1., 255.],\n",
       "        [  1.,   1.,   1.,   1., 255.],\n",
       "        [  1.,   1.,   1.,   1., 255.]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attarea=pad_F[new_min_row:new_max_row,new_min_col:new_max_col]\n",
    "attarea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_idx = torch.argmax(features)\n",
    "att_idx\n",
    "att_size = features[1] # 获取列的宽度\n",
    "att_idx = torch.cat([ (att_idx // att_size).unsqueeze(1),   #[100,1,27] 除数 实际是反flatten 代表行数\n",
    "          (att_idx  % att_size).unsqueeze(1),], dim=1)      #[100,1,27] 余数 代表列数 [4，2] 如果设计为tok 则为[4,2,tok]\n",
    "att_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "b= np.array([[1,1,1],[1,0,1],[0,1,1],[0,1,0],[1,1,1]])\n",
    "nprow =np.min(b,axis=1)\n",
    "nprow\n",
    "npcol=np.min(b,axis=0)\n",
    "npcol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mn_row=np.argmin(nprow)\n",
    "mx_row=nprow.shape[0]-np.argmin(nprow[::-1])-1\n",
    "mn_row,mx_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mn_col=np.argmin(npcol)\n",
    "mx_col=npcol.shape[0]-np.argmin(npcol[::-1])-1\n",
    "mn_col,mx_col"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gdh')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4117991765b33f5a7b6f5f48da173efdcb95991b59ec2687ae1bc171ca9e5ee4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
